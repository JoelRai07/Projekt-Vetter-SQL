# Inkonsistenzen-Check: Dokumentation vs. Code

**Datum**: Januar 2026  
**Zweck**: Systematische PrÃ¼fung aller 5 Hauptdokumente auf Inkonsistenzen mit der tatsÃ¤chlichen Code-Architektur

---

## ğŸ“‹ GeprÃ¼fte Dokumente

1. PROJEKT_ABGABE.md
2. ARCHITEKTUR_UND_PROZESSE_NEU.md
3. ARCHITEKTUR_ENTSCHEIDUNGEN.md
4. FÃ¼r_PrÃ¤si_NEU.md
5. BSL_GUIDE.md

---

## âœ… TatsÃ¤chliche Backend-Architektur (Code-RealitÃ¤t)

### Existierende Module/Dateien:
```
backend/
â”œâ”€â”€ main.py                    âœ… FastAPI App, Pipeline-Orchestrierung
â”œâ”€â”€ config.py                  âœ… Config, OPENAI_API_KEY, DEFAULT_DATABASE
â”œâ”€â”€ models.py                  âœ… Pydantic Models (QueryRequest, QueryResponse, etc.)
â”œâ”€â”€ bsl_builder.py             âœ… BSL-Generierung aus KB + Meanings
â”œâ”€â”€ database/
â”‚   â””â”€â”€ manager.py             âœ… DatabaseManager, Query-AusfÃ¼hrung, Paging
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ generator.py           âœ… OpenAIGenerator: SQL-Gen, Validation, Ambiguity
â”‚   â””â”€â”€ prompts.py             âœ… SystemPrompts (SQL_GENERATION, SQL_VALIDATION, etc.)
â””â”€â”€ utils/
    â”œâ”€â”€ cache.py               âœ… Caching fÃ¼r Schema, Meanings, Results
    â”œâ”€â”€ context_loader.py      âœ… load_context_files()
    â”œâ”€â”€ query_optimizer.py     âœ… QueryOptimizer (analyze_query_plan)
    â””â”€â”€ sql_guard.py           âœ… enforce_safety(), enforce_known_tables()
```

### NICHT existierende Module (werden manchmal erwÃ¤hnt):
- âŒ `utils/consistency_checker.py` - existiert nicht
- âŒ `llm/question_classifier.py` - existiert nicht
- âŒ Separate BSL-Module in `bsl/rules/` - BSL ist eine Textdatei, keine Python-Module

---

## ğŸ” Gefundene Inkonsistenzen

### 1. Query Optimizer ErwÃ¤hnung

**Problem**: Query Optimizer wird in manchen Dokumenten erwÃ¤hnt, aber kaum erklÃ¤rt

**Code-RealitÃ¤t**:
- âœ… Existiert: `utils/query_optimizer.py`
- âœ… Wird verwendet in `main.py` (Zeile 438-443)
- âš ï¸ Wird aber nur zur Analyse verwendet (nicht zur aktiven Optimierung)

**Betroffene Dokumente**:
- PROJEKT_ABGABE.md: Nicht explizit erwÃ¤hnt in Komponenten-Liste
- ARCHITEKTUR_UND_PROZESSE_NEU.md: Nicht in Komponenten-Tabelle
- FÃ¼r_PrÃ¤si_NEU.md: Nicht erwÃ¤hnt
- ARCHITEKTUR_ENTSCHEIDUNGEN.md: Nicht erwÃ¤hnt

**Empfehlung**: Entweder vollstÃ¤ndig dokumentieren oder als "optional/intern" markieren

---

### 2. Pipeline-Phasen Konsistenz

**Problem**: Unterschiedliche Phasen-ZÃ¤hlungen/-Namen in Dokumenten

**TatsÃ¤chlicher Ablauf** (basierend auf `main.py`):
1. Context Loading (Schema, Meanings, BSL, KB)
2. Ambiguity Detection (parallel zu SQL-Generation)
3. SQL Generation (mit integrierter Intent-Erkennung und BSL-Compliance-Checks)
4. SQL Validation (SQL Guard + LLM Validation)
5. Query Execution (mit Paging)
6. Result Summarization (optional)

**Dokumente sagen**:
- PROJEKT_ABGABE.md: "6-Phasen Pipeline" âœ… Konsistent
- ARCHITEKTUR_UND_PROZESSE_NEU.md: "6 Phasen" âœ… Konsistent
- FÃ¼r_PrÃ¤si_NEU.md: "6 Phasen" âœ… Konsistent

**Aber**: Die Phase 2/3 Benennung ist unterschiedlich:
- Manche sagen "Question Classification" â†’ sollte "Intent-Erkennung & BSL-Compliance" sein
- BSL-Generierung ist eigentlich Phase 1 (parallel zu Context Loading), nicht Phase 3

**Empfehlung**: Konsistente Phase-Namen und -Reihenfolge in allen Dokumenten

---

### 3. BSL-Generierung Timing

**Problem**: Dokumente sagen "Phase 3: BSL-Generierung", aber BSL wird vor SQL-Generation geladen

**Code-RealitÃ¤t** (`main.py` Zeile 208):
```python
kb_text, meanings_text, bsl_text = load_context_files(selected_database, Config.DATA_DIR)
```
â†’ BSL wird in Phase 1 (Context Loading) geladen, nicht erst in Phase 3

**Betroffene Dokumente**:
- Alle 5 Dokumente zeigen BSL-Generierung als separate Phase

**Empfehlung**: Klarstellen, dass BSL zur Laufzeit geladen wird (aus `credit_bsl.txt`), nicht generiert wird. Die Generierung erfolgt offline durch `bsl_builder.py`.

---

### 4. Version-Nummern Inkonsistenz

**Problem**: Unterschiedliche Version-Nummern in verschiedenen Dokumenten

**Code-RealitÃ¤t**:
- `main.py` Zeile 37: `version="2.1.0"`
- `main.py` Zeile 77: `version="2.1.0"`

**Dokumente sagen**:
- PROJEKT_ABGABE.md Zeile 6: `Version: X.0.0 (BSL-first)`
- ARCHITEKTUR_ENTSCHEIDUNGEN.md: `Version: X.0.0 (BSL-first mit modularen Regeln)`
- FÃ¼r_PrÃ¤si_NEU.md Zeile 6: `Version: X.0.0 (BSL-first)`

**Empfehlung**: Konsistente Versionsnummern (wahrscheinlich X.0.0 fÃ¼r die Dokumentation, da das die Projekt-Version ist)

---

### 5. LLM Model Name

**Problem**: Unterschiedliche Model-Namen

**Code-RealitÃ¤t**:
- `config.py`: Wird aus `.env` geladen (Config.OPENAI_MODEL)
- Dokumente erwÃ¤hnen verschiedene Model-Namen

**Dokumente sagen**:
- PROJEKT_ABGABE.md: "OpenAI GPT-5.2" âŒ (existiert nicht)
- ARCHITEKTUR_UND_PROZESSE_NEU.md: "OpenAI GPT-5.2" âŒ
- FÃ¼r_PrÃ¤si_NEU.md: "OpenAI GPT-5.2" âŒ

**Empfehlung**: Korrekte Model-Bezeichnung verwenden (z.B. "GPT-4" oder "GPT-4o" oder tatsÃ¤chlich verwendetes Modell) oder generisch "OpenAI LLM" sagen

---

### 6. "3-Level Validation" vs. "2-Level"

**Problem**: ADR-004 beschreibt "3-Level Validation", aber Level 3 existiert nicht klar als separate Ebene

**Code-RealitÃ¤t**:
- **Level 1**: `utils/sql_guard.py` - `enforce_safety()`, `enforce_known_tables()` âœ…
- **Level 2**: `llm/generator.py` - `validate_sql()` âœ…
- **Level 3**: BSL-Compliance ist Teil von Level 2, nicht separate Ebene â“

**Dokumente sagen**:
- PROJEKT_ABGABE.md ADR-004: "3 Ebenen" (Level 1, 2, 3)
- ARCHITEKTUR_ENTSCHEIDUNGEN.md: "3 Ebenen"
- Aber: PROJEKT_ABGABE.md wurde korrigiert zu "2-Level" in der Tabelle

**Empfehlung**: Konsistent als "2-Level" beschreiben, mit BSL-Compliance als Teil von Level 2

---

### 7. Cache-Struktur Details

**Problem**: Caching wird erwÃ¤hnt, aber Details fehlen

**Code-RealitÃ¤t** (`utils/cache.py`):
- `get_cached_schema()` - LRU-Cache fÃ¼r Schema
- `get_cached_meanings()` - TTL-Cache fÃ¼r Meanings
- `get_cached_query_result()` - Query-Result-Caching
- `create_query_session()`, `get_query_session()` - Session-Management fÃ¼r Paging

**Dokumente**: ErwÃ¤hnen Caching, aber Details variieren

**Empfehlung**: Konsistente Beschreibung der Cache-Arten (LRU vs. TTL)

---

### 8. Query Optimizer vs. Query Optimization

**Problem**: Query Optimizer wird verwendet, aber nicht klar, was er macht

**Code-RealitÃ¤t** (`main.py` Zeile 438-443):
```python
optimizer = QueryOptimizer(db_path)
query_plan = optimizer.analyze_query_plan(generated_sql)
if query_plan.get("full_table_scan") and query_plan.get("suggestions"):
    # Nur Hinweise ausgeben, keine aktive Optimierung
```

**Dokumente**: ErwÃ¤hnen "Query Optimization", aber nicht klar als "Analyse/Hinweise" vs. "aktive Optimierung"

**Empfehlung**: Klarstellen, dass Query Optimizer nur analysiert und Hinweise gibt, nicht aktiv optimiert

---

### 9. Self-Correction Loop

**Problem**: Self-Correction wird im Code verwendet, aber wenig dokumentiert

**Code-RealitÃ¤t** (`main.py` Zeile 312-334, 460-537):
- `generate_sql_with_correction()` wird verwendet bei niedriger Confidence oder Validation-Fehlern
- Max 2 Iterationen

**Dokumente**: ErwÃ¤hnen es teilweise, aber nicht als klare Phase/Feature

**Empfehlung**: Als Teil der SQL-Generierung dokumentieren (Self-Correction bei Problemen)

---

### 10. Temperature-Einstellungen

**Problem**: Dokumente erwÃ¤hnen `temperature=0.2` oder `temperature=0`, aber Code zeigt `temperature=0`

**Code-RealitÃ¤t** (`llm/generator.py` Zeile 36):
```python
temperature=0,
```

**Dokumente**:
- ARCHITEKTUR_ENTSCHEIDUNGEN.md: "Temperature=0.2"
- PROJEKT_ABGABE.md: Nicht explizit erwÃ¤hnt
- BSL_GUIDE.md: "temperature=0" âœ…

**Empfehlung**: Konsistent `temperature=0` dokumentieren

---

## ğŸ“Š Zusammenfassung der Inkonsistenzen

| # | Inkonsistenz | Schweregrad | Betroffene Dokumente |
|---|--------------|-------------|---------------------|
| 1 | Query Optimizer nicht in Komponenten-Listen | ğŸŸ¡ Niedrig | Alle auÃŸer BSL_GUIDE |
| 2 | Pipeline-Phasen-Namen variieren | ğŸŸ¡ Niedrig | Alle 5 |
| 3 | BSL-Generierung Timing (Phase 1 vs. Phase 3) | ğŸŸ¡ Niedrig | Alle 5 |
| 4 | Version-Nummern unterschiedlich | ğŸŸ¡ Niedrig | Alle 5 |
| 5 | LLM Model "GPT-5.2" (existiert nicht) | ğŸ”´ Hoch | PROJEKT_ABGABE, ARCHITEKTUR_UND_PROZESSE, FÃ¼r_PrÃ¤si |
| 6 | "3-Level" vs. "2-Level" Validation | ğŸŸ¡ Niedrig | PROJEKT_ABGABE, ARCHITEKTUR_ENTSCHEIDUNGEN |
| 7 | Cache-Struktur Details fehlen | ğŸŸ¢ Sehr niedrig | Alle 5 |
| 8 | Query Optimizer nur Analyse, nicht Optimierung | ğŸŸ¡ Niedrig | Alle 5 |
| 9 | Self-Correction wenig dokumentiert | ğŸŸ¡ Niedrig | Alle 5 |
| 10 | Temperature 0.2 vs. 0 | ğŸŸ¢ Sehr niedrig | ARCHITEKTUR_ENTSCHEIDUNGEN |

---

## ğŸ”§ Empfohlene Korrekturen (PrioritÃ¤t)

### ğŸ”´ Hoch-PrioritÃ¤t:
1. **LLM Model Name korrigieren**: "GPT-5.2" â†’ korrektes Modell oder generisch "OpenAI LLM"

### ğŸŸ¡ Mittel-PrioritÃ¤t:
2. **Pipeline-Phasen konsistent benennen**: Einheitliche Phase-Namen in allen Dokumenten
3. **BSL-Loading klarstellen**: BSL wird geladen, nicht generiert zur Laufzeit
4. **Version-Nummern vereinheitlichen**: Konsistente Version in allen Dokumenten
5. **Query Optimizer dokumentieren oder entfernen**: Entweder vollstÃ¤ndig dokumentieren oder als "intern" markieren

### ğŸŸ¢ Niedrig-PrioritÃ¤t:
6. **Temperature konsistent dokumentieren**: `temperature=0`
7. **Self-Correction als Feature dokumentieren**: Klar als Teil der Robustheit beschreiben

---

**Letztes Update**: Januar 2026  
**Status**: Analyse abgeschlossen, Korrekturen empfohlen
