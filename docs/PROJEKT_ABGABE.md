# Text2SQL Projekt - Abgabe Dokument

**Projekt**: ChatWithYourData - Text2SQL mit Business Semantics Layer  
**Team**: 5 Studierende der DHBW Stuttgart  
**Datum**: Januar 2026  
**Version**: X.0.0 (BSL-first)
**Success Rate**: 88.5% (7√ó100% + 3√ó95%)

---

## üìã Inhaltsverzeichnis

1. [Eingereichte Arbeitsergebnisse (Dokument)](#1-eingereichte-arbeitsergebnisse-dokument)
2. [Prototyp mit Live-Demo](#2-prototyp-mit-live-demo)
3. [Architekturdiagramm](#3-architekturdiagramm)
4. [Prozessdiagramm](#4-prozessdiagramm)
5. [Datenmodellierung & -beschreibung](#5-datenmodellierung---beschreibung)
6. [Architecture Decision Records (ADRs)](#6-architecture-decision-records-adrs)
7. [Testergebnisse](#7-testergebnisse)
8. [Limitationen der L√∂sung](#8-limitationen-der-l√∂sung)
9. [Produktivierungsanforderungen](#9-produktivierungsanforderungen)
10. [Organisatorisches](#10-organisatorisches)
11. [Selbstreflektion (Retrospektive)](#11-selbstreflektion-retrospektive)

---

## 1. Eingereichte Arbeitsergebnisse (Dokument)

Dieses Dokument enth√§lt die vollst√§ndigen Arbeitsergebnisse gem√§√ü Aufgabenstellung:
- **Prototyp mit Live-Demo**
- **Architekturdiagramm** (Komponenten + Datenfluss)
- **Prozessdiagramm** (User-Workflow)
- **Datenmodellierung & -beschreibung**
- **ADR (Architecture Decision Record)** inklusive Alternativen, Vor-/Nachteile
- **Testergebnisse**
- **Limitationen der L√∂sung**
- **Produktivierungsanforderungen**
- **Organisatorisches** (Projektplan, Rollen, Arbeitspakete)
- **Selbstreflektion (Retrospektive)** inklusive verworfener Ans√§tze

---

## 2. Prototyp mit Live-Demo

### üöÄ Demo-Zugang
- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:8000
- **Live Demo**: [Link zu Demo-Video/Pr√§sentation]

### üéØ Demo-Szenarien (4 Beispiele)

#### Szenario 1: Problem-Demo (Identifier-Verwechslung)
```
Frage: "Zeige mir digital native Kunden"
Ohne BSL: Falsche Identifier ‚Üí 0 Ergebnisse
Mit BSL: Korrekte JSON-Extraktion ‚Üí 247 Ergebnisse
```

#### Szenario 2: BSL-Regeln zeigen
```
BSL enth√§lt explizite Regeln:
- "Digital First Customer: chaninvdatablock.onlineuse = 'High'"
- "CS Format: coreregistry f√ºr Output"
- "JOIN Chain: core_record ‚Üí employment_and_income ‚Üí ..."
```

#### Szenario 3: Komplexe Query
```
Frage: "Schuldenlast nach Segment mit Prozenten"
‚Üí Multi-Level Aggregation mit CTEs
‚Üí BSL sorgt f√ºr korrekte GROUP BY + Prozentberechnung
```

#### Szenario 4: Paging & Sessions
```
Zeige query_id f√ºr konsistentes Paging
‚Üí Session Management f√ºr reproduzierbare Ergebnisse
```

### üõ†Ô∏è Technologie-Stack
- **Frontend**: React 18+ mit TypeScript, CSS
- **Backend**: Python 3.11+ mit FastAPI
- **LLM**: GPT-5.2
- **Datenbank**: SQLite (Credit Risk Domain)
- **Innovation**: Business Semantics Layer (BSL)

---

## 3. Architekturdiagramm

### üèóÔ∏è High-Level Architektur

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[React Frontend]
        UI --> |HTTP/REST| API[FastAPI Backend]
    end
    
    subgraph "Backend Layer"
        API --> |Pipeline| BB[BSL Builder]
        API --> |Pipeline| SG[SQL Generator]
        API --> |Pipeline| DM[Database Manager]
        SG --> |integriert| INT[Intent & Validation]
    end
    
    subgraph "Data Layer"
        KB[Knowledge Base]
        SCHEMA[Database Schema]
        BSL_FILE[BSL Rules]
        MEANINGS[Column Meanings]
        CACHE[Cache Layer]
    end
    
    subgraph "External Services"
        LLM[GPT-5.2]
    end
    
    SG --> LLM
    INT --> LLM
    BB --> KB
    BB --> SCHEMA
    BB --> MEANINGS
    DM --> CACHE
```

### üîß Detaillierte IT-Architektur

```mermaid
graph TB
    subgraph "Frontend [React + Vite]"
        APP[App.jsx<br/>Haupt-Komponente]
        APP --> |useState| STATE[State Management<br/>messages, question, isLoading]
        APP --> |fetch| APICALL[API Client<br/>POST /query]
        APP --> |render| UI_COMP[UI Components<br/>Messages, Table, Paging, SQL-Toggle]
    end

    subgraph "Backend [FastAPI + Python 3.11+]"
        subgraph "API Layer"
            MAIN[main.py<br/>FastAPI App]
            MAIN --> |imports| MODELS[models.py<br/>Pydantic Models]
            MODELS --> QR[QueryRequest]
            MODELS --> QRESP[QueryResponse]
            MODELS --> AR[AmbiguityResult]
            MODELS --> VR[ValidationResult]
        end

        subgraph "LLM Layer"
            GEN[generator.py<br/>OpenAIGenerator]
            GEN --> |uses| PROMPTS[prompts.py<br/>SystemPrompts]
            PROMPTS --> P_AMB[AMBIGUITY_DETECTION]
            PROMPTS --> P_SQL[SQL_GENERATION]
            PROMPTS --> P_VAL[SQL_VALIDATION]
            PROMPTS --> P_SUM[RESULT_SUMMARY]
            GEN --> |methods| M1[check_ambiguity]
            GEN --> |methods| M2[generate_sql]
            GEN --> |methods| M3[validate_sql]
            GEN --> |methods| M4[summarize_results]
            GEN --> |methods| M5[generate_sql_with_correction]
            GEN --> |helper| BSL_COMP[BSL Compliance Trigger<br/>_is_property_leverage_question<br/>_is_digital_engagement_cohort_question<br/>_bsl_compliance_instruction]
        end

        subgraph "Utils Layer"
            CACHE[cache.py]
            CACHE --> C1[get_cached_schema<br/>LRU, permanent]
            CACHE --> C2[meanings_cache<br/>TTL 1h]
            CACHE --> C3[query_cache<br/>TTL 5min]
            CACHE --> C4[query_session_cache<br/>TTL 1h]

            GUARD[sql_guard.py]
            GUARD --> G1[enforce_safety<br/>nur SELECT erlaubt]
            GUARD --> G2[enforce_known_tables<br/>Tabellenvalidierung]

            CTX[context_loader.py]
            OPT[query_optimizer.py]
        end

        subgraph "Database Layer"
            DBM[manager.py<br/>DatabaseManager]
            DBM --> |methods| DB1[get_schema_and_sample]
            DBM --> |methods| DB2[get_table_columns]
            DBM --> |methods| DB3[execute_query_with_paging]
            DBM --> |methods| DB4[normalize_sql_for_paging]
        end

        subgraph "Offline Tools"
            BSLB[bsl_builder.py<br/>BSL Generator]
        end
    end

    subgraph "Data Layer"
        subgraph "SQLite Database"
            DB[(credit.sqlite)]
            DB --> T1[core_record]
            DB --> T2[employment_and_income]
            DB --> T3[expenses_and_assets]
            DB --> T4[bank_and_transactions]
            DB --> T5[credit_and_compliance]
            DB --> T6[credit_accounts_and_history]
        end

        subgraph "Knowledge Files [JSON/Text]"
            KB_FILE[credit_kb.jsonl<br/>Knowledge Base]
            MEAN_FILE[credit_column_meaning_base.json<br/>Spalten-Bedeutungen]
            BSL_FILE[credit_bsl.txt<br/>Business Semantics Layer]
        end

        subgraph "Configuration Files"
            ENV[.env<br/>OPENAI_API_KEY<br/>OPENAI_MODEL]
            CONFIG[config.py<br/>DATA_DIR, DEFAULT_DATABASE<br/>MAX_RESULT_ROWS]
        end
    end

    subgraph "External Services"
        OPENAI[OpenAI API<br/>GPT Model]
    end

    subgraph "Frameworks & Dependencies"
        direction LR
        subgraph "Backend"
            PY[Python 3.11+]
            FA[FastAPI]
            UV[Uvicorn]
            PYD[Pydantic]
            OAI[openai]
            CT[cachetools]
            DOT[python-dotenv]
        end
        subgraph "Frontend"
            REACT[React 19]
            VITE[Vite 7]
            ESL[ESLint]
            PRET[Prettier]
        end
        subgraph "Database"
            SQL[SQLite 3]
        end
    end

    %% Connections
    APICALL --> MAIN
    MAIN --> GEN
    MAIN --> CACHE
    MAIN --> GUARD
    MAIN --> CTX
    MAIN --> OPT
    MAIN --> DBM
    GEN --> OPENAI
    DBM --> DB
    CTX --> KB_FILE
    CTX --> MEAN_FILE
    CTX --> BSL_FILE
    BSLB --> KB_FILE
    BSLB --> MEAN_FILE
    BSLB --> DB
    BSLB --> BSL_FILE
    MAIN --> CONFIG
```

#### Dateistruktur

```
backend/
‚îú‚îÄ‚îÄ main.py                 # FastAPI Entry Point, Request-Orchestrierung
‚îú‚îÄ‚îÄ models.py               # Pydantic Models (Request/Response)
‚îú‚îÄ‚îÄ config.py               # Konfiguration (API Keys, Pfade)
‚îú‚îÄ‚îÄ bsl_builder.py          # Offline-Tool: BSL-Generierung
‚îú‚îÄ‚îÄ test_questions.py       # Test-Suite
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îú‚îÄ‚îÄ generator.py        # OpenAIGenerator (alle LLM-Interaktionen)
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py          # System Prompts (Ambiguity, SQL, Validation, Summary)
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ manager.py          # DatabaseManager (SQLite-Zugriff, Paging)
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ cache.py            # Multi-Layer Caching (Schema, Meanings, Query, Session)
‚îÇ   ‚îú‚îÄ‚îÄ sql_guard.py        # Sicherheits-Guards (Safety, Tables)
‚îÇ   ‚îú‚îÄ‚îÄ context_loader.py   # L√§dt KB, Meanings, BSL
‚îÇ   ‚îî‚îÄ‚îÄ query_optimizer.py  # Query Plan Analyse
‚îî‚îÄ‚îÄ mini-interact/credit/
    ‚îú‚îÄ‚îÄ credit.sqlite       # SQLite Datenbank
    ‚îú‚îÄ‚îÄ credit_kb.jsonl     # Knowledge Base
    ‚îú‚îÄ‚îÄ credit_column_meaning_base.json  # Spalten-Bedeutungen
    ‚îî‚îÄ‚îÄ credit_bsl.txt      # Generierter BSL (von bsl_builder.py)

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.jsx            # React Entry Point
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx             # Haupt-Komponente (UI + State + API)
‚îÇ   ‚îú‚îÄ‚îÄ App.css             # Styling
‚îÇ   ‚îî‚îÄ‚îÄ index.css           # Global Styles
‚îú‚îÄ‚îÄ index.html              # HTML Template
‚îî‚îÄ‚îÄ vite.config.js          # Vite Konfiguration
```

#### Komponenten-Verantwortlichkeiten

| Komponente | Datei | Verantwortlichkeit |
|------------|-------|-------------------|
| **FastAPI App** | `main.py` | Request-Handling, Pipeline-Orchestrierung, Error Handling |
| **Pydantic Models** | `models.py` | Type-safe Request/Response Definitionen |
| **OpenAIGenerator** | `llm/generator.py` | Alle LLM-Aufrufe (Ambiguity, SQL, Validation, Summary), BSL-Compliance Checks |
| **SystemPrompts** | `llm/prompts.py` | Zentrale Prompt-Definitionen f√ºr jeden LLM-Task |
| **DatabaseManager** | `database/manager.py` | Schema-Extraktion, Query-Ausf√ºhrung, Paging |
| **Cache Module** | `utils/cache.py` | 4-Layer Caching (Schema, Meanings, Query, Session) |
| **SQL Guard** | `utils/sql_guard.py` | Sicherheitspr√ºfung (nur SELECT), Tabellenvalidierung |
| **Context Loader** | `utils/context_loader.py` | L√§dt KB, Meanings, BSL aus Dateien |
| **Query Optimizer** | `utils/query_optimizer.py` | EXPLAIN Query Plan Analyse, Index-Empfehlungen |
| **BSL Builder** | `bsl_builder.py` | Offline-Tool: Generiert BSL aus KB + Meanings + Schema |
| **React App** | `frontend/src/App.jsx` | UI, State Management, API-Kommunikation |

### üîÑ Request-Flow Pipeline

> **Wichtig**: `bsl_builder.py` ist ein **Build-/Maintenance-Tool** (offline/on-demand) und **kein** Request-Step im API-Flow. Die BSL-Datei (`credit_bsl.txt`) wird zur Laufzeit nur geladen, nicht generiert.

```mermaid
sequenceDiagram
  participant User
  participant FE as Frontend
  participant API as FastAPI
  participant GEN as OpenAIGenerator
  participant DB as DatabaseManager
  participant LLM as OpenAI
  participant GUARD as Server Guards

  User->>FE: Frage + Paging Parameter
  FE->>API: POST /query

  API->>API: Load Schema/Meanings/KB/BSL (cached)

  par Parallel LLM Calls
    API->>GEN: check_ambiguity(question, schema, kb, meanings)
    GEN->>LLM: Ambiguity prompt
    LLM-->>GEN: ambiguity JSON
  and
    API->>GEN: generate_sql(question, schema, meanings, bsl)
    GEN->>LLM: SQL generation prompt (BSL-first)
    LLM-->>GEN: SQL JSON
  end

  API->>API: If confidence < 0.4 -> generate_sql_with_correction()
  API->>GUARD: enforce_safety + enforce_known_tables
  API->>GEN: validate_sql(sql, schema)
  GEN->>LLM: Validation prompt
  LLM-->>GEN: validation JSON
  API->>API: If severity high -> correction loop + re-validate

  API->>DB: execute_query_with_paging
  DB-->>API: results + paging_info

  API->>GEN: summarize_results(...)
  GEN->>LLM: Summary prompt
  LLM-->>GEN: Summary text

  API-->>FE: Response (sql, results, paging, validation, ambiguity, summary)
  FE-->>User: Anzeige
```

### üìä Komponenten & Datenfluss

| Komponente | Verantwortlichkeit | Datenfluss |
|------------|-------------------|------------|
| **React Frontend** | UI, Frage-Input, Ergebnisanzeige | HTTP ‚Üí Backend |
| **FastAPI Backend** | Pipeline-Orchestrierung, Caching, Server Guards | Koordiniert Request-Flow |
| **BSL Builder (Offline-Tool)** | Generiert `credit_bsl.txt` aus KB + Meanings | KB + Meanings ‚Üí BSL (einmalig) |
| **LLM Generator** | BSL-first SQL-Generierung, heuristische Fragetyp-Erkennung (Pattern-Matching), SQL-Validation (integriert), Summaries | BSL + Schema ‚Üí SQL |
| **SQL Guard** | Security (nur SELECT), Tabellenvalidierung | SQL ‚Üí Validated SQL |
| **Database Manager** | Query-Ausf√ºhrung, Paging, Sessions | SQL ‚Üí Results |

---

## 4. Prozessdiagramm

### üë§ User Workflow durch das Tool

```mermaid
flowchart TD
    A[User stellt Frage] --> B[Frontend sendet POST /query]
    B --> C{query_id vorhanden?}

    %% Paging-Pfad (mit existierender Session)
    C -->|Ja| D[Session aus Cache laden]
    D --> E[Server Guards pr√ºfen]
    E --> F[Query mit Paging ausf√ºhren]
    F --> RESP[Response an Frontend]

    %% Neuer Query-Pfad
    C -->|Nein| G{Cache Hit?}
    G -->|Ja| RESP

    G -->|Nein| H[Context Loading<br/>Schema, KB, Meanings, BSL]
    H --> I[PARALLEL:<br/>Ambiguity Check + SQL Generation]

    I --> J{Confidence < 0.4?}
    J -->|Ja| K[Self-Correction Loop<br/>max 2 Iterationen]
    K --> L
    J -->|Nein| L[Server Guards<br/>Safety + Tabellenvalidierung]

    L --> M{Guards OK?}
    M -->|Nein| N[Autokorrektur versuchen]
    N --> O{Immer noch Fehler?}
    O -->|Ja| ERR1[Error Response]
    O -->|Nein| P
    M -->|Ja| P[LLM SQL Validation]

    P --> Q{Severity = high?}
    Q -->|Ja| R[Correction Loop + Re-Validate]
    R --> S{Immer noch high?}
    S -->|Ja| ERR2[Error Response]
    S -->|Nein| T
    Q -->|Nein| T[Query ausf√ºhren]

    T --> U[Result Summarization]
    U --> V[Session erstellen + Cache speichern]
    V --> RESP

    %% Frontend zeigt Ergebnisse
    RESP --> W[Ergebnisse anzeigen]
    W --> X{Weitere Seite?}
    X -->|Ja| Y[Request mit query_id]
    Y --> B
    X -->|Nein| Z[Ende]
```

### üîÑ Detail-Prozessablauf

> **Wichtig**: `bsl_builder.py` ist ein **Offline/On-demand Tool** und **kein** Request-Step. Die BSL-Datei wird zur Laufzeit nur geladen.

#### Haupt-Request-Flow (neue Frage)

| Schritt | Bezeichnung | Beschreibung |
|---------|-------------|--------------|
| **Phase 0** | Build/Maintenance (offline) | BSL-Generierung durch `bsl_builder.py` (nicht pro Request) |
| **1** | Cache-Check | Pr√ºft ob identische Frage bereits im Cache ist ‚Üí direkter Return |
| **2** | Context Loading | Schema, Meanings, KB, BSL werden geladen (cached) |
| **3** | Parallel LLM Calls | Ambiguity Detection + SQL-Generierung laufen parallel |
| **4** | Self-Correction (optional) | Bei Confidence < 0.4: bis zu 2 Korrektur-Iterationen |
| **5** | Server Guards | `enforce_safety` + `enforce_known_tables` + ggf. Autokorrektur |
| **6** | LLM SQL Validation | Semantische Pr√ºfung, bei Severity "high" ‚Üí Korrektur + Re-Validate |
| **7** | Query Execution | SQL ausf√ºhren mit Paging |
| **8** | Result Summarization | LLM fasst Ergebnisse zusammen |
| **9** | Session + Cache | Session f√ºr Paging erstellen, Ergebnis cachen |

#### Paging-Flow (mit query_id)

| Schritt | Bezeichnung | Beschreibung |
|---------|-------------|--------------|
| **1** | Session laden | SQL aus Session-Cache holen (kein LLM-Aufruf!) |
| **2** | Server Guards | Sicherheitspr√ºfung der gespeicherten SQL |
| **3** | Query Execution | SQL mit neuem Page-Offset ausf√ºhren |
| **4** | Response | Ergebnisse zur√ºckgeben (ohne Summarization) |

#### Wie heuristische Fragetyp-Erkennung in diesem Projekt funktioniert:

**Kein separater Intent-Classifier**, sondern **Pattern-Matching f√ºr BSL-Compliance** in `llm/generator.py`:

1. **Implizite Intent-Erkennung**: Das LLM erkennt den Intent direkt beim SQL-Generieren (z.B. "nach Segment" ‚Üí Aggregation, "top 10" ‚Üí Ranking)
2. **Pattern-basierte BSL-Compliance-Trigger (Layer A)**: Helper-Funktionen erkennen spezifische Frage-Patterns:
   - `_is_property_leverage_question()`: "property leverage", "mortgage ratio", "LTV"
   - `_is_digital_engagement_cohort_question()`: Cohort-basierte Engagement-Fragen
   - `_has_explicit_time_range()`: Explizite Jahres-/Quartals-Angaben
   - Diese Trigger **aktivieren BSL-Regel-Verst√§rkungen**, sind aber **keine hardcodierten SQL-Antworten**
3. **BSL-Compliance-Regeneration**: `_bsl_compliance_instruction` ‚Üí `_regenerate_with_bsl_compliance` bei Verst√∂√üen

> **Wichtig**: Pattern-Funktionen verst√§rken nur relevante BSL-Regeln im Prompt. Das LLM generiert immer dynamisch SQL basierend auf vollst√§ndigem BSL + Schema + Meanings Kontext.

---

## 5. Datenmodellierung & -beschreibung

### üóÑÔ∏è Datenbank-Schema (Credit DB)

```mermaid
erDiagram
    CORE_RECORD ||--|| EMPLOYMENT_AND_INCOME : links
    EMPLOYMENT_AND_INCOME ||--|| EXPENSES_AND_ASSETS : links
    EXPENSES_AND_ASSETS ||--|| BANK_AND_TRANSACTIONS : links
    BANK_AND_TRANSACTIONS ||--|| CREDIT_AND_COMPLIANCE : links
    CREDIT_AND_COMPLIANCE ||--|| CREDIT_ACCOUNTS_AND_HISTORY : links

    CORE_RECORD {
        string coreregistry PK
        string clientref
        string clientseg
        date scoredate
        string risklev
        float custlifeval
        int tenureyrs
    }

    EMPLOYMENT_AND_INCOME {
        string emplcoreref PK
        string coreregistry FK
        float mthincome
        float debincratio
        float credutil
    }

    EXPENSES_AND_ASSETS {
        string exemplref PK
        string emplcoreref FK
        float totassets
        float totliabs
        float liqassets
        string propfinancialdata
    }

    BANK_AND_TRANSACTIONS {
        string bankexpref PK
        string exemplref FK
        string chaninvdatablock
    }

    CREDIT_AND_COMPLIANCE {
        string compbankref PK
        string bankexpref FK
        int delinqcount
        int latepaycount
    }

    CREDIT_ACCOUNTS_AND_HISTORY {
        string histcompref PK
        string compbankref FK
        int acctcount
        int openacctcount
        int closedacctcount
    }
```

### üìä Daten-Beziehungen & Business Logik

#### Kern-Entit√§ten
- **CORE_RECORD**: Kundendaten mit Identifikatoren und Risikoinformationen
- **EMPLOYMENT_AND_INCOME**: Einkommens- und Besch√§ftigungsdaten
- **EXPENSES_AND_ASSETS**: Verm√∂gens- und Ausgabendaten
- **BANK_AND_TRANSACTIONS**: Banktransaktionen und Kanalnutzung
- **CREDIT_AND_COMPLIANCE**: Kredit- und Compliance-Daten

#### Wichtige Business Rules
1. **Dual Identifier System**: 
   - `coreregistry` (CS) f√ºr Business-Output (customer_id) und JOINs
   - `clientref` (CU) nur wenn explizit nach client reference/clientref gefragt

2. **Strikte FK-Kette**: JOINs m√ºssen der Foreign-Key-Kette folgen
   ```
   core_record ‚Üí employment_and_income ‚Üí expenses_and_assets 
   ‚Üí bank_and_transactions ‚Üí credit_and_compliance ‚Üí credit_accounts_and_history
   ```

3. **JSON-Felder**: Strukturierte Daten in bestimmten Tabellen
   - `propfinancialdata` in expenses_and_assets
   - `chaninvdatablock` in bank_and_transactions

### üß† Business Semantics Layer (BSL)

#### BSL-Sektionen (in generierter `credit_bsl.txt`)

Die BSL-Regeln werden durch `bsl_builder.py` generiert und als **Sektionen in einer Textdatei** gespeichert:

1. **Identity System Rules**: CU vs CS Identifier System
2. **Aggregation Patterns**: GROUP BY vs ORDER BY + LIMIT
3. **Business Logic Rules**: Financially Vulnerable, High-Risk, etc.
4. **Join Chain Rules**: Strikte Foreign-Key Chain
5. **JSON Field Rules**: JSON-Extraktionsregeln
6. **Complex Query Templates**: Multi-Level Aggregation, CTEs

> **Hinweis**: Diese sind Textbl√∂cke im generierten BSL-File, keine separaten `.py`-Dateien.

#### BSL-Inhalt (Beispiele)
```
# IDENTITY SYSTEM RULES
## ‚ö†Ô∏è CRITICAL: Dual Identifier System
- CS Format: coreregistry (for customer_id output and JOINs)
- CU Format: clientref (only when explicitly requested as client reference)

# AGGREGATION PATTERNS
## Aggregation vs Detail Queries
- "by category", "by segment" ‚Üí GROUP BY
- "top N", "highest" ‚Üí ORDER BY + LIMIT

# BUSINESS LOGIC RULES
## Financial Vulnerability
- debincratio > 0.5 AND liqassets < mthincome √ó 3
```

---

## 6. Architecture Decision Records (ADRs)

> **Hinweis**: Die ADRs folgen dem **MADR-Template** (Markdown Architecture Decision Record) gem√§√ü Aufgabenstellung.
> F√ºr vollst√§ndige ADRs siehe `docs/ARCHITEKTUR_ENTSCHEIDUNGEN.md`

### ADR Index

| ADR | Titel | Status | Date | Superseded by |
|-----|-------|--------|------|---------------|
| ADR-001 | Initiale Multi-Database RAG/ReAct Architektur | deprecated | 12.01.2026 | ADR-004 |
| ADR-002 | Database Auto-Routing | deprecated | 12.01.2026 | ADR-004 |
| ADR-003 | Vector Store (ChromaDB) | deprecated | 12.01.2026 | ADR-004 |
| ADR-004 | Migration zu BSL-first Single-Database | accepted | 12.01.2026 | ‚Äì |
| ADR-005 | Heuristische Fragetyp-Erkennung + BSL-Compliance-Trigger | accepted | 12.01.2026 | ‚Äì |
| ADR-006 | Consistency Validation (mehrstufig) | accepted | 12.01.2026 | ‚Äì |
| ADR-007 | Multi-Layer Caching Strategie | accepted | 17.01.2026 | ‚Äì |

---

### ADR-001: Initiale Multi-Database RAG/ReAct Architektur

| | |
|---|---|
| **Status** | deprecated ‚Äì superseded by [ADR-004](#adr-004-migration-zu-bsl-first-single-database-architektur) |
| **Deciders** | Tim K√ºhne, Dominik Ruoff, Joel Martinez, Umut Polat, S√∂ren Frank |
| **Date** | 15.12.2025 (created) / 12.01.2026 (deprecated) |
| **Technical Story** | Architekturentwurf f√ºr BIRD-INTERACT (Multi-DB) mit Token-Optimierung durch Retrieval |

#### Context and Problem Statement

Zu Projektbeginn wurde eine Architektur angestrebt, die Multi-Database-Support f√ºr den BIRD-Datensatz erm√∂glicht und Token-Kosten durch intelligentes Retrieval reduziert. Die Frage war: Wie k√∂nnen wir ein skalierbares Text2SQL-System f√ºr mehrere Datenbanken bauen?

#### Decision Drivers

- Token-Effizienz (Kontext-Overload vermeiden)
- Multi-DB Support (Benchmark umfasst viele DBs)
- Moderne Retrieval-Methodik (RAG/ReAct)

#### Considered Options

1. RAG + ReAct mit Vector Store
2. Full-Context Prompting
3. Fine-Tuned Model

#### Decision Outcome

Chosen option: **"RAG + ReAct mit Vector Store"**, because es Token-Reduktion und Multi-DB-Support versprach.

#### Consequences

- **Good**: Token-Reduktion durch Retrieval, Multi-DB prinzipiell m√∂glich
- **Bad**: Nicht-deterministische Ergebnisse, hohe Komplexit√§t, schwer zu debuggen

> **Grund f√ºr Deprecation**: Bei Tests zeigten sich Identity Leakage, Aggregation Failures und Semantic Drift. Siehe ADR-004 f√ºr Details.

---

### ADR-002: Database Auto-Routing

| | |
|---|---|
| **Status** | deprecated ‚Äì superseded by [ADR-004](#adr-004-migration-zu-bsl-first-single-database-architektur) |
| **Deciders** | Tim K√ºhne, Dominik Ruoff, Joel Martinez |
| **Date** | 15.12.2025 (created) / 12.01.2026 (deprecated) |
| **Technical Story** | Automatische DB-Auswahl per LLM zur Unterst√ºtzung von Multi-DB |

#### Context and Problem Statement

F√ºr Multi-DB wurde ein Mechanismus ben√∂tigt, der automatisch bestimmt, welche Datenbank zur Frage passt ‚Äì ohne manuelle Auswahl.

#### Decision Outcome

LLM-basiertes Auto-Routing via DB-Profil-Snippets + Confidence-Threshold (‚â•0.55).

#### Consequences

- **Bad**: +2-3s Latenz pro Request, nicht-deterministisch, Over-Engineering f√ºr Single-DB Scope

> **Grund f√ºr Deprecation**: Projekt-Scope fokussiert auf Credit-DB; Routing war unn√∂tige Komplexit√§t.

---

### ADR-003: Vector Store (ChromaDB)

| | |
|---|---|
| **Status** | deprecated ‚Äì superseded by [ADR-004](#adr-004-migration-zu-bsl-first-single-database-architektur) |
| **Deciders** | Tim K√ºhne, Dominik Ruoff |
| **Date** | 15.12.2025 (created) / 12.01.2026 (deprecated) |
| **Technical Story** | Persistenter Vector Store zur Token-Reduktion und semantischen Chunk-Suche |

#### Context and Problem Statement

Schema-/KB-Inhalte sollten als Chunks gespeichert und semantisch durchsucht werden, um Prompt-L√§nge zu reduzieren.

#### Decision Outcome

ChromaDB als lokaler, persistenter Vector Store.

#### Consequences

- **Bad**: Dateikorruption m√∂glich (erlebt), Maintenance-Aufwand, nicht-deterministisches Retrieval

> **Grund f√ºr Deprecation**: Vector Store wurde korrupt, Retrieval-Qualit√§t schwankte, f√ºr Single-DB nicht sinnvoll.

---

### ADR-004: Migration zu BSL-first Single-Database Architektur

| | |
|---|---|
| **Status** | accepted |
| **Deciders** | Tim K√ºhne, Dominik Ruoff, Joel Martinez, Umut Polat, S√∂ren Frank |
| **Date** | 12.01.2026 |
| **Technical Story** | Nach initialer RAG/ReAct-Implementierung zeigte sich bei Tests instabile Ergebnisse. Professor-Feedback empfahl BSL als besseren Ansatz f√ºr den Credit-DB Scope. |

#### Context and Problem Statement

Die initiale Text2SQL-Architektur (Version 1.0.0-7.0.0) basierte auf RAG (Retrieval Augmented Generation) mit ReAct-Loop und ChromaDB als Vector Store. Bei der Evaluation mit 10 Testfragen zeigten sich kritische Probleme:

1. **Identity Leakage**: CU und CS Identifier wurden inkonsistent verwendet (Q1-Q5, Q9)
2. **Aggregation Failure**: GROUP BY fehlte bei Aggregationsfragen (Q4)
3. **Semantic Drift**: Business Rules wurden falsch interpretiert (Q6, Q7)
4. **Nicht-deterministische Ergebnisse**: Gleiche Fragen produzierten unterschiedliche SQL

Die zentrale Frage war: Wie erreichen wir reproduzierbare und auditierbare SQL-Generierung f√ºr die Credit-DB?

#### Decision Drivers

- **Stabilit√§t**: Deterministische Ergebnisse f√ºr Evaluation erforderlich
- **Nachvollziehbarkeit**: Explizite Business Rules statt impliziter Embeddings
- **Wartbarkeit**: Weniger Dependencies und Moving Parts
- **Scope-Fit**: Projekt fokussiert auf Credit-Datenbank (BIRD mini-interact Subset)
- **Professor-Feedback**: BSL als "bester Ansatz" explizit empfohlen
- **Academic Rigor**: Nachvollziehbare Architektur f√ºr Verteidigung

#### Considered Options

1. **RAG + ReAct beibehalten** (Status Quo)
2. **Hybrid-Ansatz** (RAG + BSL kombiniert)
3. **BSL-first** (vollst√§ndige Migration)

#### Decision Outcome

Chosen option: **"Option 3: BSL-first"**, because es erf√ºllt alle kritischen Anforderungen (Stabilit√§t, Nachvollziehbarkeit, Wartbarkeit), implementiert Professor-Feedback direkt, reduziert Komplexit√§t signifikant und bietet eine bessere Grundlage f√ºr akademische Verteidigung.

#### Positive Consequences

- Deterministische SQL-Generierung: Gleiche Frage + gleicher BSL = gleiche SQL
- Explizite, auditierbare Business Rules: BSL ist Plain-Text, Domain-Experten k√∂nnen pr√ºfen
- Weniger Dependencies: Kein ChromaDB, LangChain, Vector Store
- Einfachere Wartung und Debugging: Klare Fehlerquellen, keine "Black Box"
- Success Rate von 40% auf 88.5% verbessert

#### Negative Consequences

- H√∂here Token-Kosten: ~32KB vs ~2KB pro Prompt
- Weniger skalierbar: Multi-DB-Support erfordert pro-DB BSL

#### Pros and Cons of the Options

**Option 1: RAG + ReAct beibehalten**

| Pro | Contra |
|-----|--------|
| Geringere Token-Kosten (~2KB) | Nicht-deterministische Ergebnisse |
| Skalierbar f√ºr gro√üe Schemas | Hohe Komplexit√§t (ChromaDB, LangChain) |
| | Schwer zu debuggen und zu auditieren |

**Option 2: Hybrid-Ansatz (RAG + BSL)**

| Pro | Contra |
|-----|--------|
| Flexible Kombination | Komplexit√§t bleibt hoch |
| Token-Effizienz f√ºr gro√üe Schemas | Unklare Priorit√§t (wann RAG, wann BSL?) |

**Option 3: BSL-first (chosen)**

| Pro | Contra |
|-----|--------|
| Deterministisch und reproduzierbar | Hoher Token-Verbrauch (~32KB) |
| Explizite, auditierbare Regeln | Enger Domain-Fit (nur Credit-DB) |
| SOLID-Prinzipien, wartbar | |

#### Links

- Supersedes: ADR-001, ADR-002, ADR-003
- Related: ADR-005, ADR-006

---

### ADR-005: Heuristische Fragetyp-Erkennung + BSL-Compliance-Trigger

| | |
|---|---|
| **Status** | accepted |
| **Deciders** | Tim K√ºhne, Dominik Ruoff, Joel Martinez |
| **Date** | 12.01.2026 |
| **Technical Story** | F√ºr robuste Text2SQL musste das System auf Frage-Variationen generalisieren, ohne hardcodierte SQL-Antworten. |

#### Context and Problem Statement

F√ºr eine robuste Text2SQL-Pipeline war eine Strategie erforderlich, die:
- Das LLM bei der korrekten Anwendung von BSL-Regeln unterst√ºtzt
- Auf Variationen von Fragen generalisiert (z.B. "property leverage" ‚Üí "mortgage ratio" ‚Üí "LTV")
- **Keine** fertigen SQL-Antworten pro Frage enth√§lt (kein Hardcoding)

Die Frage war: Wie k√∂nnen wir Edge Cases abfangen, ohne das Generalisierungsziel zu kompromittieren?

#### Decision Drivers

- **Generalizability**: System muss auf Frage-Variationen korrekt reagieren
- **BSL Compliance**: LLM muss die richtigen BSL-Regeln anwenden
- **Maintainability**: Erweiterbar f√ºr neue Dom√§nen-Konzepte
- **Robustness**: Edge Cases m√ºssen abgefangen werden
- **Academic Rigor**: Kein Hardcoding von Frage-Antwort-Paaren

#### Considered Options

1. **Reines LLM** ohne zus√§tzliche Unterst√ºtzung
2. **Hardcodierte SQL** pro Frage-Typ
3. **LLM + Keyword-basierte BSL Compliance Trigger**

#### Decision Outcome

Chosen option: **"Option 3: LLM + Keyword-basierte BSL Compliance Trigger"**, because es Generalisierung erm√∂glicht, w√§hrend Edge Cases durch Regel-Verst√§rkung abgefangen werden.

**Wichtige Klarstellung - Kein Hardcoding:**

| Was sie NICHT tun | Was sie tun |
|-------------------|-------------|
| Fertige SQL-Queries zur√ºckgeben | BSL-Regeln aktivieren/verst√§rken |
| Frage-Antwort-Paare speichern | Dem LLM signalisieren, welche Regeln wichtig sind |
| Das LLM umgehen | Das LLM mit zus√§tzlichem Kontext unterst√ºtzen |

**Technische Implementierung (2 Stufen):**

1. **Initial SQL-Generierung**: LLM erkennt Intent direkt im Prompt
2. **BSL-Compliance-Check**: Pattern-basierte Helper-Funktionen erkennen Edge Cases und triggern ggf. Regeneration

**Beispiel Pattern-Funktionen** (in `llm/generator.py`):
- `_is_property_leverage_question()`: Erkennt "property leverage", "mortgage ratio", "LTV"
- `_is_digital_engagement_cohort_question()`: Erkennt "cohort" + "engagement" + "digital"
- `_has_explicit_time_range()`: Erkennt explizite Jahres-/Quartals-Angaben

#### Positive Consequences

- LLM generiert SQL immer dynamisch basierend auf BSL + Schema + Meanings
- Edge Cases werden durch Regel-Verst√§rkung abgefangen
- System generalisiert auf Frage-Variationen

#### Negative Consequences

- Etwas komplexere Code-Struktur in `generator.py`
- Trigger-Logik muss f√ºr neue Dom√§nen erweitert werden

#### Pros and Cons of the Options

**Option 1: Reines LLM ohne Unterst√ºtzung**

| Pro | Contra |
|-----|--------|
| Maximale Einfachheit | Edge Cases werden nicht zuverl√§ssig erkannt |
| | BSL-Regeln k√∂nnten ignoriert werden |

**Option 2: Hardcodierte SQL pro Frage-Typ**

| Pro | Contra |
|-----|--------|
| 100% deterministisch | Keine Generalisierung |
| | Akademisch nicht vertretbar |

**Option 3: LLM + BSL Compliance Trigger (chosen)**

| Pro | Contra |
|-----|--------|
| Generalisierung + Robustheit | Zus√§tzliche Trigger-Logik erforderlich |
| Nachvollziehbar und erweiterbar | |

---

### ADR-006: Consistency Validation (mehrstufig)

| | |
|---|---|
| **Status** | accepted |
| **Deciders** | Tim K√ºhne, Joel Martinez, S√∂ren Frank |
| **Date** | 12.01.2026 |
| **Technical Story** | Nach BSL-Migration zeigte sich, dass LLMs trotz BSL-Regeln weiterhin Fehler machten: Identifier-Verwechslungen, JOIN-Chain-Verletzungen, Aggregationsfehler. |

#### Context and Problem Statement

Nach der BSL-Migration (ADR-004) verbesserte sich die Accuracy signifikant. Jedoch machte das LLM trotz BSL-Regeln weiterhin Fehler:
- **Identifier-Verwechslungen** (CU vs CS) in 5% der F√§lle
- **JOIN-Chain-Verletzungen** (Tabellen √ºbersprungen)
- **Aggregationsfehler** (GROUP BY fehlend bei "by segment")
- **JSON-Feld-Qualifizierungsprobleme** (falsche Tabelle.Spalte)

Wie k√∂nnen wir diese Fehler systematisch erkennen und beheben?

#### Decision Drivers

- **Quality Assurance**: Automatische Fehlererkennung vor Ausf√ºhrung
- **BSL Consistency**: BSL-Regeln m√ºssen durchgesetzt werden
- **Debugging**: Klare Fehlermeldungen f√ºr Entwickler
- **Defense in Depth**: Mehrere Validierungsebenen
- **Performance**: Validation muss schnell sein (<500ms)

#### Considered Options

1. **Nur LLM-basierte Validierung**
2. **Nur Rule-based Validierung** (Regex)
3. **Mehrstufige Validation** (3 Ebenen)

#### Decision Outcome

Chosen option: **"Option 3: Mehrstufige Validation"**, because es Defense in Depth bietet und verschiedene Fehlerklassen auf unterschiedlichen Ebenen erkennt.

**Die 3-Ebenen Validierungs-Architektur:**

| Ebene | Typ | Pr√ºft | Speed | Implementierung |
|-------|-----|-------|-------|-----------------|
| **Layer A** | Rule-based + Auto-repair | BSL-Compliance, SQLite Dialektfix | ~10ms | `llm/generator.py` |
| **Server Guards** | SQL Guard + Known Tables | Sicherheit, Tabellenvalidierung | ~10ms | `utils/sql_guard.py`, `main.py` |
| **Layer B** | LLM Validation | Semantik, JOINs, Spalten-Existenz | ~1-2s | `llm/generator.py` |

#### Positive Consequences

- Umfassende Fehlererkennung (Sicherheit + Semantik + BSL)
- Klare Fehlermeldungen mit Severity-Level
- Defense in Depth - mehrere Schichten

#### Negative Consequences

- Zus√§tzliche Latenz (~2-3s f√ºr vollst√§ndige Validation bei Layer B)

#### Pros and Cons of the Options

**Option 1: Nur LLM-basierte Validierung**

| Pro | Contra |
|-----|--------|
| Versteht Semantik und Kontext | Langsam (~2s) f√ºr einfache Checks |
| | Kann Sicherheitsprobleme √ºbersehen |

**Option 2: Nur Rule-based Validierung**

| Pro | Contra |
|-----|--------|
| Schnell (~10ms), deterministisch | Versteht keine Semantik |
| | Kann BSL-Compliance nicht pr√ºfen |

**Option 3: Mehrstufige Validation (chosen)**

| Pro | Contra |
|-----|--------|
| Beste Abdeckung aller Fehlerklassen | Komplexere Implementierung |
| Defense in Depth | |

---

### ADR-007: Multi-Layer Caching Strategie

| | |
|---|---|
| **Status** | accepted |
| **Deciders** | Tim K√ºhne, Dominik Ruoff, Joel Martinez |
| **Date** | 17.01.2026 |
| **Technical Story** | Performance-Optimierung und Kostenreduktion durch intelligentes Caching |

#### Context and Problem Statement

Das Text2SQL System f√ºhrt bei jeder Anfrage mehrere ressourcenintensive Operationen durch:
- Schema-Ladung aus Datenbank (langsame I/O-Operationen)
- Dom√§nenwissen-Ladung aus Dateien (JSON-Parsing, File-Reading)
- LLM-Aufrufe f√ºr SQL-Generierung (teuer, 3-5 Sekunden Latenz)
- Komplette Pipeline-Ausf√ºhrung (mehrere Validierungsschritte)

Ohne Caching w√ºrde jede identische Frage erneut die komplette Pipeline durchlaufen, was zu:
- Hoen Latenz bei wiederholten Fragen
- Unn√∂tigen API-Kosten
- Schlechter User Experience
- Ineffizienter Ressourcennutzung

#### Decision Drivers

1. **Performance-Optimierung**: Reduzierung von Antwortzeiten bei wiederholten Fragen
2. **Kostenreduktion**: Weniger OpenAI API-Costs durch Wiederverwendung von Ergebnissen
3. **User Experience**: Schnellere Antworten bei h√§ufigen Fragen
4. **Ressourcen-Effizienz**: Entlastung von Datenbank und LLM-Diensten
5. **Skalierbarkeit**: Bessere Performance bei hoher Last

#### Considered Options

1. **Kein Caching**: Jede Anfrage l√§uft durch komplette Pipeline
2. **Einfacher Result-Cache**: Nur finale Ergebnisse cachen
3. **Multi-Layer Caching**: Unterschiedliche Cache-Strategien pro Datentyp

#### Decision Outcome

Chosen option: **"Multi-Layer Caching"**, because es unterschiedliche TTL-Anforderungen ber√ºcksichtigt und maximale Performance-Vorteile bietet.

#### Cache-Architektur

| Cache-Typ | Zweck | TTL | Gr√∂√üe | Technologie | Key-Strategie |
|-----------|-------|-----|-------|-------------|----------------|
| **Schema Cache** | Datenbank-Schemas | Unlimitiert (LRU) | 32 DBs | `@lru_cache` | Datenbank-Pfad |
| **Meanings Cache** | Dom√§nenwissen | 1 Stunde | 32 Eintr√§ge | `TTLCache` | `{db_name}_meanings` |
| **Query Cache** | Komplette Query-Ergebnisse | 5 Minuten | 100 Queries | `TTLCache` | MD5(`{question}_{database}`) |
| **Session Cache** | Paging-Sessions | 1 Stunde | 200 Sessions | `TTLCache` | UUID (Session-ID) |

#### Technische Implementierung

**Phase 1: Cache-Check (vor Pipeline)**
```python
# Pr√ºfung auf Cache-Hit vor kompletter Pipeline
cached_result = get_cached_query_result(request.question, selected_database)
if cached_result and request.page == 1:
    print("‚úÖ Cache Hit - verwende gecachtes Ergebnis")
    return QueryResponse(**cached_result)
```

**Phase 2: Context-Caching (w√§hrend Pipeline)**
```python
# Wiederverwendung von gecachten Daten
schema = get_cached_schema(db_path)  # LRU, permanent
meanings_text = get_cached_meanings(selected_database, Config.DATA_DIR)  # 1h TTL
```

**Phase 3: Result-Caching (nach Pipeline)**
```python
# Nur bei Seite 1 cachen (f√ºr Cache-Hits)
if request.page == 1:
    cache_query_result(request.question, selected_database, result_dict)
```

#### Positive Consequences

- **Performance**: Cache-Hits in <100ms statt 3-5 Sekunden Pipeline
- **Kostenersparnis**: Bis zu 80% weniger LLM-Aufrufe bei wiederholten Fragen
- **Skalierbarkeit**: Bessere Performance bei hoher Last
- **Monitoring**: `/cache-status` Endpoint f√ºr Transparenz und Debugging
- **User Experience**: Deutlich schnellere Antworten bei h√§ufigen Fragen

#### Negative Consequences

- **Speicherverbrauch**: Caches ben√∂tigen RAM (konfigurierbare Gr√∂√üen)
- **Stale Data**: M√∂gliche veraltete Ergebnisse bei Daten√§nderungen
- **Komplexit√§t**: Zus√§tzliche Cache-Management-Logik
- **Cache-Invalidation**: Manuelle Invalidierung bei Schema-√Ñnderungen notwendig

#### Cache-Strategien im Detail

**Schema Cache (Permanent):**
- Schemas √§ndern sich selten ‚Üí dauerhaftes Caching sinnvoll
- LRU-Verwaltung mit maxsize=32 f√ºr verschiedene Datenbanken
- Keine TTL notwendig, da Schema-√Ñnderungen Backend-Neustart erfordern

**Meanings Cache (1 Stunde):**
- Dom√§nenwissen √§ndert sich gelegentlich ‚Üí kurze TTL
- Vermeidet wiederholtes File-Reading und JSON-Parsing
- Balance zwischen Frische der Daten und Performance

**Query Cache (5 Minuten):**
- Ergebnisse √§ndern sich potenziell h√§ufig ‚Üí kurze TTL
- Komplette Response-Objekte f√ºr sofortige R√ºckgabe
- MD5-Hash als Key f√ºr effiziente Lookups

**Session Cache (1 Stunde):**
- Paging-Sessions f√ºr konsistente Navigation
- UUID-basiert f√ºr parallele User-Sessions
- L√§ngere TTL f√ºr typische User-Session-Dauer

#### Monitoring & Management

**Cache-Status Endpoint:**
```python
@app.get("/cache-status")
async def cache_status():
    return {
        "meanings_cache": {"size": len(meanings_cache), "ttl": 3600},
        "query_cache": {"size": len(query_cache), "ttl": 300},
        "session_cache": {"size": len(query_session_cache), "ttl": 3600},
        "schema_cache": get_cached_schema.cache_info()
    }
```

**Performance-Metriken:**
- Cache-Hit-Rate: Ziel >60% bei typischer Last
- Antwortzeiten: <100ms bei Cache-Hits
- Kostenreduktion: Bis zu 80% bei wiederholten Fragen

#### Links

- Related: ADR-004 (BSL-first Architektur)
- Related: ADR-006 (Consistency Validation)
- Implementation: `backend/utils/cache.py`

---

## 7. Testergebnisse

### üìä Success Rate: 88.5% (7√ó100% + 3√ó95%)

| Frage | Typ                                   | Erwartetes Verhalten                                                             | Status Core | BSL-Regeln                     |
|-------|---------------------------------------|----------------------------------------------------------------------------------|-------------|--------------------------------|
| Q1    | Ranking / Wealth (Top-N)             | Top-10 Kunden nach Net Worth (Assets ‚àí Liabilities) inkl. IDs, Werte & Ranking    | ‚úÖ 100 %     | Identity, Join Chain           |
| Q2    | Digital Segmentierung (Rule-based)  | CS-IDs gem√§√ü Digital-First-Regel (JSON-Extraktion, definierte Kriterien)          | ‚úÖ 100 %     | Aggregation, Time Logic        |
| Q3    | Investment Segmentierung (Rule-based)| CS-ID + Investmentbetrag + Assets f√ºr Investment-fokussierte Kunden              | ‚úÖ 100 %     | Aggregation, Business Logic    |
| Q4    | Credit-Score-Klassifikation (Aggregiert) | Kategorie-Summary: Credit-Kategorie + COUNT + AVG                              | ‚úÖ 100 %     | Aggregation Patterns           |
| Q5    | Property Leverage / LTV (Row-level, JSON) | CS-ID + Property Value + Mortgage + Ratio als JSON-Feld                        | ‚úÖ 100 %     | JSON Rules, Identity           |
| Q6    | Risikoklassifizierung (Rule-based)   | Klassifikation gem√§√ü Business-Regeln (regelbasierte Ableitung)                   | ‚úÖ 95 %      | Business Logic                 |
| Q7    | Multi-Level Aggregation              | CTEs + Prozentberechnung √ºber mehrere Ebenen                                     | ‚úÖ 100 %     | Complex Templates              |
| Q8    | Segment-√úbersicht (Total)            | UNION ALL f√ºr Segment-Summen                                     | ‚ö†Ô∏è 60 %      | Complex Templates              |
| Q9    | Property Leverage (Tabellenspezifisch) | Tabellen-spezifische Regeln                                 | ‚úÖ 100 %     | Business Logic                 |
| Q10   | Kredit-Details (Detail-Query)        | Row-Level-Details, kein GROUP BY                                                 | ‚úÖ 95 %      | Aggregation Patterns           |


### üéØ Validierungs-Performance

**Manuelle Evaluationsergebnisse (basierend auf 10 Testfragen):**
- **Identifier Consistency**: 95% Korrektheit (1 Fehler bei Q6 und Q10)
- **Mehr ausgegebene Spalten als gefragt**: 95% Korrektheit (1 Fehler bei Q6)
- **Fehlende Spalten**: 60% Korrektheit (1 Fehler bei Q8)
- **JOIN Chain Validation**: 100% Korrektheit
- **Aggregation Logic**: 100% Korrektheit
- **BSL Compliance**: 100% Korrektheit
- **Overall Success Rate**: 95% (7√ó100% + 2√ó95% + 1x60%)

> **Hinweis**: Diese Metriken sind manuelle Evaluationsergebnisse aus der Analyse der 10 Testfragen. Die SQL-Validation erfolgt durch `validate_sql()` in `backend/llm/generator.py` (integriert, **kein separates** `consistency_checker.py` Modul).

**Performance-Charakteristik:**
- **Antwortzeit**: Schneller als beim alten RAG + ReAct Ansatz (keine Retrieval-Latenz)
- **Token-Verbrauch**: H√∂her als RAG-Ansatz (BSL-first ben√∂tigt vollst√§ndigen Kontext)
- **Trade-off**: Stabilit√§t und Determinismus gegen Token-Kosten
- **Validation-Time**: <500ms f√ºr SQL-Validation

### üî¨ Evaluationsmethode

**Qualit√§tsindikatoren:**
1. **SQL-Korrektheit**: Syntax und Semantik
2. **Ergebnis-Korrektheit**: Richtige Daten zur√ºckgegeben
3. **BSL-Compliance**: Business Rules befolgt
4. **Performance**: Antwortzeit und Ressourcenverbrauch
5. **Reproduzierbarkeit**: Gleiche Frage = gleiche SQL

---

## 8. Limitationen der L√∂sung

### üîß Technische Limitationen

1. **Single-Database-Fokus**: Nur Credit-Datenbank unterst√ºtzt
   - Multi-DB-Support w√ºrde pro-DB BSL und Routing erfordern
   - Aktuelle Architektur ist auf Credit-DB optimiert

2. **Token-Kosten**: Hohe Anzahl an Tokens pro Prompt durch BSL-first Ansatz
   - H√∂her als RAG-Ansatz (~2KB), aber f√ºr Credit-DB akzeptabel
   - Trade-off: Stabilit√§t > Token-Effizienz

3. **SQLite-Skalierung**: Nicht f√ºr High-Concurrency-Szenarien optimiert
   - Connection Pooling erforderlich f√ºr Produktion
   - Index-Strategie-Optimierung notwendig

4. **LLM-Abh√§ngigkeit**: Externe API erforderlich
   - Network Latency und API-Limits
   - Kostenfaktor bei intensiver Nutzung

### üìä Funktionale Limitationen

1. **Einfache JOINs**: Nur komplexe Foreign-Key-Chains
   - Keine Ad-hoc JOINs √ºber Tabellenketten hinweg
   - JOIN-Logik ist strikt an Schema gebunden

2. **Statische Metriken**: Keine dynamische Berechnungen zur Laufzeit
   - Metriken sind in BSL fest kodifiziert
   - Benutzerdefinierte Berechnungen nicht m√∂glich

3. **Begrenzte Aggregation**: Keine Window Functions oder CTEs f√ºr komplexe Analysen
   - Grundlegende Aggregationen unterst√ºtzt
   - Erweiterte SQL-Features fehlen

4. **Keine Prozeduren**: Nur SELECT-Statements, keine Stored Procedures
   - Sicherheitsentscheidung (Read-Only)
   - DDL-Operationen nicht m√∂glich

### üéØ Scope-Limitationen

1. **Domain-Spezifisch**: Optimiert f√ºr Credit Risk Domain
   - BSL-Regeln sind credit-spezifisch
   - Generalisierung auf andere Dom√§nen erfordert Neuentwicklung

2. **Frage-Typen**: Getestet auf 10 spezifische Fragen
   - Erfolgsrate bei allgemeinen Fragen unklar
   - Edge Cases nicht vollst√§ndig abgedeckt

---

## 9. Produktivierungsanforderungen

### üîß Technische Anforderungen

1. **Multi-Database-Support**
   - Pro Datenbank eigenes BSL
   - Database-Routing-Layer
   - Zentrales BSL-Management
   - **Aufwand**: Hoch (Neuentwicklung Routing + BSL-Generation)

2. **Performance-Optimierung**
   - Connection Pooling f√ºr SQLite
   - Query Result Caching mit Redis/Memcached
   - Index-Strategie-Optimierung
   - **Aufwand**: Mittel (Best Practices)

3. **Security Hardening**
   - User Authentication & Authorization (OAuth2/JWT)
   - Rate Limiting und API Quotas
   - Audit Logging f√ºr Compliance
   - **Aufwand**: Mittel (Standard-Implementierung)

4. **Monitoring & Observability**
   - Structured Logging (ELK-Stack)
   - Performance Metrics (Prometheus + Grafana)
   - Error Tracking und Alerting (Sentry)
   - **Aufwand**: Mittel (Infrastruktur)

### üé® Funktionale Anforderungen

1. **Erweiterte SQL-Unterst√ºtzung**
   - Window Functions f√ºr komplexe Analysen
   - Recursive CTEs f√ºr hierarchische Daten
   - Stored Procedures (Read-Only) f√ºr h√§ufige Queries
   - **Aufwand**: Mittel (SQL-Erweiterungen)

2. **User Experience**
   - Query History und Favoriten
   - Export-Functions (CSV, Excel, PDF)
   - Visual Query Builder f√ºr Nicht-Techniker
   - **Aufwand**: Hoch (UX-Entwicklung)

3. **Admin-Funktionen**
   - BSL-Editor mit Live-Preview
   - Schema-Management und Versionierung
   - User Management und Berechtigungen
   - **Aufwand**: Hoch (Admin-Interface)

### üè¢ Organisatorische Anforderungen

1. **Compliance & Governance**
   - GDPR-konforme Datenverarbeitung
   - Data Retention Policies
   - Audit Trail f√ºr alle Query-Ausf√ºhrungen
   - **Aufwand**: Mittel (Rechtliche Anforderungen)

2. **Training & Documentation**
   - Benutzerhandbuch und Video-Tutorials
   - Admin-Dokumentation
   - BSL-Authoring Guidelines
   - **Aufwand**: Niedrig (Dokumentation)

3. **Support & Wartung**
   - 24/7 Monitoring und Alerting
   - Backup- und Recovery-Strategien
   - Versionierungs-Management f√ºr BSL
   - **Aufwand**: Mittel (Operations)

### ‚è±Ô∏è Zeitplan f√ºr Produktivierung

| Phase | Dauer | Hauptaufgaben | Erfolgsfaktoren |
|-------|--------|---------------|----------------|
| **Phase 1** | 4-6 Wochen | Multi-DB Support, BSL-Management | Architektur-Entscheidungen |
| **Phase 2** | 3-4 Wochen | Security Hardening, Monitoring | Security-Expertise |
| **Phase 3** | 4-6 Wochen | UX-Verbesserungen, Admin-Tools | Frontend-Ressourcen |
| **Phase 4** | 2-3 Wochen | Testing, Documentation, Deployment | QA-Team |

**Gesamtaufwand**: 13-19 Wochen (3-5 Monate)

---

## 10. Organisatorisches

### üë• Team-Struktur

```mermaid
graph TD
    PL[Project Lead<br/>Tim K√ºhne] --> ARCH[System Architecture]
    PL --> COORD[Project Coordination]

    PL --> BE[Backend Developer<br/>Dominik Ruoff]
    PL --> FS[Fullstack Developer<br/>Joel Martinez]
    PL --> PE[Prompt Engineer<br/>Umut Polat]
    PL --> QA[QA & Documentation<br/>S√∂ren Frank]

    BE --> LLM[LLM Integration]
    BE --> DB[Database Management]
    BE --> RAG[ReAct + RAG]

    FS --> FE[Frontend UI]
    FS --> BSL[BSL Integration]
    FS --> RAG[ReAct + RAG Optimization]

    PE --> PROMPT[Few-Shot Prompting]
    PE --> PE2[Prompt Engineering]

    QA --> TEST[Testing]
    QA --> DOCS[Documentation]
    QA --> DEPLOY[Deployment]
```

### üìã Team-Mitglieder

| Rolle | Person | Verantwortlichkeiten |
|-------|--------|-------------------|
| **Project Lead** | Tim K√ºhne | Gesamtprojekt-Koordination, Architektur | 
| **Backend Developer** | Dominik Ruoff | LLM Integration, Database Management | 
| **Fullstack Developer** | Joel Martinez | Frontend UI, BSL-Integration, ReAct + RAG optimizer | 
| **Prompt Engineer** | Umut Polat | Few-Shot Prompting, Prompting, Promptengineering | 
| **QA & Documentation** | S√∂ren Frank | Testing, Dokumentation, Deployment | 

### üìÖ Arbeitspakete & Zeitplan

| Arbeitspaket                                              | Art des Tickets | Bezeichnung | Verantwortlich                          | Dauer    | Status | Aufwand |
|----------------------------------------------------------|-----------------|-------------|-----------------------------------------|----------|--------|---------|
| Architektur & Projektsetup                               | Task            | Proj-7      | Tim K√ºhne                               | 5 Tage   | Fertig | 3,5 Std |
| Projekt-Repository initialisieren                        | Task            | Proj-8      | Tim K√ºhne                               | 1 Tag    | Fertig | 0,5 Std |
| Datenmodell analysieren                                  | Task            | Proj-9      | Joel Martinez                           | 2 Tage   | Fertig | 0,5 Std |
| Datenmodellierung und -beschreibung                      | Task            | Proj-10     | S√∂ren Frank                             | 2 Tage   | Fertig | 1 Std   |
| React-Frontend + Features                                | Task            | Proj-11     | Joel Martinez                           | 5 Tage   | Fertig | 2,5 Std |
| Recherche der Ans√§tze des Backends                       | Task            | Proj-12     | Dominik Ruoff & Joel Martinez           | 4 Tage   | Fertig | 4 Std   |
| Dokumentation des Frontends                              | Task            | Proj-13     | S√∂ren Frank                             | 1 Tag    | Fertig | 1,5 Std |
| Database Manager                                         | Task            | Proj-14     | Dominik Ruoff                           | 1 Tag    | Fertig | 1 Std   |
| Anbindung an das LLM                                     | Task            | Proj-15     | Dominik Ruoff                           | 1 Tag    | Fertig | 1 Std   |
| Schema-RAG                                               | Task            | Proj-16     | Dominik Ruoff & Joel Martinez           | 5 Tage   | Fertig | 3 Std   |
| Prompt-Engineering Recherche                             | Task            | Proj-17     | Umut Polat                              | 2 Tage   | Fertig | 1,5 Std |
| Few-Shot Prompting / In-Context Examples                 | Task            | Proj-18     | Umut Polat                              | 3 Tage   | Fertig | 2 Std   |
| Erstellung eines pr√§zisen Prompts                        | Task            | Proj-19     | Umut Polat                              | 4 Tage   | Fertig | 1,5 Std |
| ReAct-Agent mit Tool execute_sql implementieren          | Task            | Proj-20     | Dominik Ruoff & Joel Martinez           | 4 Tage   | Fertig | 3 Std   |
| Confidence-Mechanismus                                   | Task            | Proj-21     | Joel Martinez                           | 2 Tage   | Fertig | 2 Std   |
| SQL Guards und Sicherheit                                | Task            | Proj-22     | Dominik Ruoff & Joel Martinez           | 2 Tage   | Fertig | 1,5 Std |
| Multi-DB Routing                                         | Task            | Proj-23     | Joel Martinez                           | 2 Tage   | Fertig | 2 Std   |
| Technische Dokumentation Modellierungen                  | Task            | Proj-24     | S√∂ren Frank                             | 10 Tage  | Fertig | 5 Std   |
| Qualit√§t der Text2SQL-Ergebnisse                         | Task            | Proj-25     | Joel Martinez                           | 3 Tage   | Fertig | 2 Std   |
| Query-basiertes RAG                                      | Task            | Proj-26     | Dominik Ruoff                           | 2 Tage   | Fertig | 3 Std   |
| Caching                                                  | Task            | Proj-27     | Dominik Ruoff & Joel Martinez           | 2 Tage   | Fertig | 2 Std   |
| Testing der Ergebnisse und Vergleich                     | Task            | Proj-28     | S√∂ren Frank                             | 2 Tage   | Fertig | 2 Std   |
| Vector-Store aufsetzen                                   | Task            | Proj-29     | Joel Martinez                           | 4 Tage   | Fertig | 2,5 Std |
| Verwerfung des Ansatzes & neue Architektur entwerfen    | Task            | Proj-30     | Tim K√ºhne                               | 5 Tage   | Fertig | 5 Std   |
| L√∂schen alter Dateien und verbleibender Inhalte          | Task            | Proj-31     | Joel Martinez                           | 1 Tag    | Fertig | 1,5 Std |
| BSL Builder implementieren + BSL-Versionierung           | Task            | Proj-32     | Joel Martinez                           | 2 Tage   | Fertig | 3 Std   |
| Prompt und Pipeline an BSL anpassen                      | Task            | Proj-33     | Umut Polat                              | 2 Tage   | Fertig | 3 Std   |
| BSL Compliance Checker + Auto-Repair Rules               | Task            | Proj-34     | Dominik Ruoff & Joel Martinez           | 2 Tage   | Fertig | 3 Std   |
| First Round of Testing (neue Architektur)               | Task            | Proj-35     | S√∂ren Frank                             | 2 Tage   | Fertig | 2 Std   |
| Implementierung automatischer Tests                      | Task            | Proj-36     | Joel Martinez                           | 1 Tag    | Fertig | 0,5 Std |
| Output Schema Enforcer (nur gefragte Spalten)           | Task            | Proj-37     | Joel Martinez                           | 1 Tag    | Fertig | 2 Std   |
| Identifier Output Guard (CS default, CU nur explizit)   | Task            | Proj-38     | Joel Martinez                           | 1 Tag    | Fertig | 1,5 Std |
| JSON Extraction Validator                                 | Task            | Proj-39     | Dominik Ruoff                           | 1 Tag    | Fertig | 2 Std   |
| Second Round of Testing (neue Architektur)               | Task            | Proj-40     | S√∂ren Frank                             | 1 Tag    | Fertig | 1 Std   |
| Determinism Test (gleiche Frage ‚Üí gleiche SQL)           | Task            | Proj-41     | S√∂ren Frank                             | 1 Tag    | Fertig | 0,5 Std |
| SQL Injection Test                                       | Task            | Proj-42     | S√∂ren Frank                             | 1 Tag    | Fertig | 0,5 Std |
| Doku-Konsistenz verbessern                               | Task            | Proj-43     | Tim K√ºhne                               | 0,5 Tage | Fertig | 4 Std   |
| Projektmanagement & Retrospektive                        | Task            | Proj-44     | Tim K√ºhne                               | 1 Tag    | Fertig | 2 Std   |
| Pr√§sentation gestalten                                   | Task            | Proj-45     | Tim K√ºhne, S√∂ren Frank, Umut Polat      | 4 Tage   | Fertig | 10 Std  |


**Gesamtaufwand**: 90 Stunden (ca. 10 Wochen bei 9 Stunden pro Woche)

### üîÑ Projektmethodik

**Agile Entwicklung mit Scrum:**
- **Sprint-L√§nge**: 2 Wochen
- **Daily Standups**: An der DHBW ab und zu besprochen
- **Sprint Reviews**: Ende jeder Sprint-Woche
- **Retrospektive**: Nach jedem Sprint
- **Tools**: GitHub Projects, Kanban Board, Jira

**Kommunikation:**
- **W√∂chentliches Team-Meeting**: Montags 18:30
- **Dokumentation**: Confluence + GitHub
- **Code Reviews**: Pull Requests f√ºr alle √Ñnderungen

---

## 11. Selbstreflektion (Retrospektive)

### ‚úÖ Was gut funktioniert hat

1. **Fr√ºhes Professor-Feedback**: BSL-Ansatz war entscheidend f√ºr Erfolg
   - Direkte Integration des Feedbacks vermeidet Fehlentwicklungen
   - Professor-Feedback als "bester Ansatz" best√§tigt Richtung

2. **Modulare Architektur**: BSL-Sektionen machen Wartung und Testing einfach
   - BSL als Textdatei mit klaren Sektionen (Part A / Part B / Annex C)
   - Unabh√§ngige Anpassungen und Erweiterungen m√∂glich

3. **Deterministische Ergebnisse**: Reproduzierbarkeit f√ºr Evaluation entscheidend
   - Gleiche Frage + gleicher BSL = gleiche SQL
   - Wichtig f√ºr akademische Verteidigung und Produktion

4. **Explicit over Implicit**: BSL-Regeln sind besser als implizite Embeddings
   - Regeln sind auditierbar und nachvollziehbar
   - Domain-Experten k√∂nnen BSL pr√ºfen

5. **Scope-Fit**: Single-DB-Fokus vermeidet Over-Engineering
   - YAGNI-Prinzip erfolgreich angewendet
   - Fokus auf Credit-DB statt Multi-DB-Generalisierung

6. **Team-Kollaboration**: Klare Verantwortlichkeiten und parallele Arbeit
   - Gute Kommunikation und regelm√§√üige Syncs
   - Effiziente Arbeitsteilung nach St√§rken

### ‚ö†Ô∏è Was wir im Nachhinein anders machen w√ºrden

1. **Fr√ºhere Testing-Phase**: Mehr Unit Tests f√ºr einzelne Module
   - Tests f√ºr BSL-Module von Anfang an
   - Automatisierte Regression-Tests einf√ºhren
   - **Lerne**: Qualit√§tssicherung von Anfang an priorisieren

2. **Performance-Optimierung**: Fr√ºhere Beachtung von Token-Kosten
   - Monitoring von Anfang an implementieren
   - Caching-Strategie fr√ºher entwickeln
   - **Lerne**: Nicht-funktionale Anforderungen fr√ºh ber√ºcksichtigen

3. **Error Handling**: Robustere Fehlerbehandlung von Anfang an
   - Try-Catch-Bl√∂cke f√ºr alle kritischen Komponenten
   - User-freundliche Fehlermeldungen
   - **Lerne**: Robustheit ist kein nachtr√§glicher Zusatz

4. **Dokumentation**: Kontinuierliche Dokumentation statt nachtr√§glicher Aufarbeitung
   - ADRs w√§hrend der Entwicklung schreiben
   - README und API-Docs aktuell halten
   - **Lerne**: Dokumentation ist lebendes Dokument

5. **CI/CD Pipeline**: Automatisiertes Testing und Deployment
   - GitHub Actions f√ºr automatische Tests
   - Deployment-Pipeline f√ºr Staging/Production
   - **Lerne**: Automatisierung reduziert manuelle Fehler

### üß™ Verworfene Ans√§tze (Begr√ºndung & Erfahrungen)

1. **RAG/ReAct mit Vector Store**
   - **Warum verworfen**: Instabile Ergebnisse, hoher Infrastrukturaufwand (ChromaDB, LangChain), unn√∂tig f√ºr Single-DB-Scope.
   - **Wie es funktionierte**: Embeddings der Tabellenbeschreibungen wurden genutzt, um Kontext zu fetchen; LLM generierte SQL mit ReAct-Schritten.
   - **Lesson Learned**: F√ºr kleine, stabile Schemas ist explizite Regelmodellierung robuster als Retrieval.

2. **Hybrid-Ansatz (RAG + BSL)**
   - **Warum verworfen**: Kombiniert die Komplexit√§ten beider Welten ohne klare Vorteile f√ºr die Credit-DB.
   - **Wie es funktionierte**: Retrieval f√ºr Kontext, BSL f√ºr kritische Regeln; f√ºhrte zu inkonsistenten Prompt-L√§ngen und Debugging-Aufwand.
   - **Lesson Learned**: Ein klarer, einfacher Architekturpfad schl√§gt "Best-of-both-worlds" in engen Scopes.

3. **Reines Prompt-Engineering ohne BSL**
   - **Warum verworfen**: Fehlende Auditierbarkeit und wiederkehrende Fehler bei Identifiers und JOINs.
   - **Wie es funktionierte**: System-Prompt mit Schema und Guidelines, ohne modulare Regeln.
   - **Lesson Learned**: Domain-Regeln m√ºssen explizit modelliert sein, nicht implizit im Prompt.

### üîç Identifizierte Probleme & L√∂sungen (Testing-Phase)

Die folgenden Probleme wurden w√§hrend der Evaluation der 10 Testfragen identifiziert und durch die BSL-Migration gel√∂st:

#### Problem 1: Identity Leakage (CU vs CS)

| Aspekt | Details |
|--------|---------|
| **Symptom** | CU-Identifier (z.B. `CU154870`) statt CS-Identifier (z.B. `CS239090`) ausgegeben |
| **Betroffene Fragen** | Q1, Q2, Q3, Q5, Q9 |
| **Root Cause** | Datenbank verwendet zwei Identifier pro Person: CU (Customer ID) und CS (Core Registry ID) |
| **L√∂sung** | BSL-Regel "Identity System Rules" mit expliziter CU/CS-Dokumentation |

#### Problem 2: Aggregation Failure

| Aspekt | Details |
|--------|---------|
| **Symptom** | Einzelne Zeilen statt aggregierte Werte (z.B. bei "nach Kategorie") |
| **Betroffene Fragen** | Q4 (Credit Category Aggregation) |
| **Root Cause** | LLM erkannte Aggregations-Intent nicht, lieferte Row-Level-Daten |
| **L√∂sung** | BSL-Sektion "Aggregation Patterns" mit Regel: "by category/segment" ‚Üí GROUP BY |

#### Problem 3: Semantic Drift (Business Rules)

| Aspekt | Details |
|--------|---------|
| **Symptom** | Falsche Spaltenauswahl, fehlende Filter, Business Rules ignoriert |
| **Betroffene Fragen** | Q6 (FSI nicht gefiltert), Q7 (Time-Series statt Cohort Comparison) |
| **Root Cause** | Implizite Business-Begriffe (z.B. "Financial Stress Indicator") wurden falsch interpretiert |
| **L√∂sung** | BSL-Sektion "Business Logic Rules" mit expliziten Definitionen und Formeln |

#### Problem 4: Join Drift

| Aspekt | Details |
|--------|---------|
| **Symptom** | Falsche Zeilen/IDs durch unn√∂tige JOINs √ºber mehrere Tabellen |
| **Betroffene Fragen** | Q5 (Property Leverage) |
| **Root Cause** | LLM jointe `core_record` ‚Üí `employment_and_income` ‚Üí `expenses_and_assets`, obwohl alle Daten in einer Tabelle lagen |
| **L√∂sung** | BSL-Regel: "Wenn alle Felder in einer Tabelle, keine zus√§tzlichen JOINs" + "Join Chain Rules" |

#### Problem 5: SQL Execution Bug (Parameter Binding)

| Aspekt | Details |
|--------|---------|
| **Symptom** | SQL mit Platzhaltern (`?`, `:param`) statt konkreten Werten |
| **Betroffene Fragen** | Q6, Q8 |
| **Root Cause** | LLM generierte parametrisierte Queries statt ausf√ºhrbare SQL |
| **L√∂sung** | Layer A Validation mit `_contains_param_placeholders()` Check + Auto-Repair |

#### Zusammenfassung der Fehlerklassen

| Fehlerklasse | H√§ufigkeit (vor BSL) | Nach BSL-Migration |
|--------------|---------------------|-------------------|
| Identity Leakage | 50% der Fragen | 5% (nur Edge Cases) |
| Aggregation Failure | 10% der Fragen | 0% |
| Semantic Drift | 20% der Fragen | 5% |
| Join Drift | 10% der Fragen | 0% |
| Parameter Binding | 20% der Fragen | 0% |

> **Dokumentation**: Die vollst√§ndigen Test-Analysen befinden sich in `docs/archiv/Probleme/TESTING.md` und `docs/archiv/Probleme/TESTING2.0.md`.

### üéì Lessons Learned

1. **Scope-Fit ist kritisch**: Multi-DB-Support war Over-Engineering
   - YAGNI-Prinzip bew√§hrt sich
   - Fokus auf tats√§chlichen Anforderungen statt "was k√∂nnte man brauchen"

2. **Stabilit√§t > Optimierung**: Deterministische Ergebnisse wichtiger als Token-Effizienz
   - F√ºr Evaluation und Produktion ist Reproduzierbarkeit entscheidend
   - Trade-off bewusst getroffen und dokumentiert

3. **Explicit > Implicit**: Explizite BSL-Regeln besser als implizite Embeddings
   - Auditierbarkeit und Nachvollziehbarkeit sind Premium-Features
   - "Black Box" Ans√§tze sind f√ºr akademische Arbeit ungeeignet

4. **Modularit√§t zahlt sich aus**: Bessere Wartbarkeit und Testbarkeit
   - SOLID-Prinzipien sind keine akademischen √úbungen
   - Gute Architektur zahlt sich langfristig aus

5. **Fr√ºhes Feedback einholen**: Professor-Integration war entscheidend f√ºr Erfolg
   - Externe Perspektiven vermeiden "Tunnel Vision"
   - Expertise nutzen statt gegen den Strom zu schwimmen

### üöÄ N√§chste Schritte & Empfehlungen

1. **Produktivierung**: Umsetzung der in Kapitel 8 beschriebenen Anforderungen
2. **Multi-DB-Erweiterung**: Wenn Bedarf besteht, Architektur entsprechend anpassen
3. **Performance-Tuning**: Basierend auf Produktivierungs-Erfahrungen optimieren
4. **User Testing**: Mit echten Nutzern Feedback sammeln und umsetzen
5. **Open Source LLM**: √úberlegen ein Fine-Tuning-LLM anstatt GPT-5.2 zu benutzen

---

*Dieses Dokument enth√§lt alle geforderten Arbeitsergebnisse gem√§√ü der Aufgabenstellung (50 Punkte) und wurde sorgf√§ltig vorbereitet f√ºr die akademische Bewertung.*
