@startuml ablauf_tech

' =================== STYLING (Technical View) ===================
skinparam backgroundColor #f5f5f5
skinparam sequenceArrowColor #1976d2
skinparam actorBackgroundColor #bbdefb
skinparam actorBorderColor #1976d2
skinparam participantBackgroundColor #e3f2fd
skinparam participantBorderColor #1976d2
skinparam noteBkgColor #f3e5f5
skinparam noteBorderColor #7b1fa2
skinparam noteTextColor #1a1a1a

title Text2SQL System - Technischer Ablauf (Developer View)

' =================== PARTICIPANTS ===================
actor User
participant "App.jsx\n(React)" as frontend
participant "main.py\n(FastAPI)" as backend
participant "generator.py\n(LLM)" as llm
participant "schema_retriever.py\n(RAG)" as retriever
participant "cache.py\n(Cache)" as cache
participant "sql_guard.py\n(Security)" as guard
participant "manager.py\n(DB)" as db
participant "Chroma\n(Vector Store)" as vector
participant "OpenAI API\n(GPT-5.2)" as openai

' =================== FLOW ===================

user ->> frontend: input: question, database, page, page_size
note right of user: z.B. "Premium-Kunden mit hoher DTI pro Segment"

activate frontend
frontend ->> backend: POST /query(QueryRequest)
deactivate frontend

activate backend

note over backend: Phase 1: Cache & Context Loading
backend ->> cache: get_cached_schema(db_name)
activate cache
cache -->> backend: schema_text | null
deactivate cache

alt schema_text == null (5% Cache Miss Rate)
  backend ->> retriever: retrieve_schema(db_name)\n[500ms]
  activate retriever
  note right of retriever: Split schema in chunks (500 tokens each)
  
  retriever ->> vector: similarity_search(\n  question_embedding,\n  top_k=5)
  activate vector
  note right of vector: ChromaDB Vector DB
  vector ->> openai: POST /embeddings\n  model: text-embedding-3-small
  openai -->> vector: embedding_vectors
  vector -->> retriever: [{chunk, score}, ...]
  deactivate vector
  
  retriever -->> backend: schema_text (2KB relevant chunks)
  deactivate retriever
  
  backend ->> cache: cache_schema(db_name, schema, ttl=1h)
  activate cache
  cache -->> backend: âœ“ LRU Cache
  deactivate cache
else Cache Hit (95% Case)
  backend ->> cache: cache.get_schema(db_name)
  activate cache
  cache -->> backend: schema_text [10ms]
  deactivate cache
end

note over backend: **Context Assembly** (Parallel loads)

backend ->> cache: get_cached_kb(db_name)
activate cache
cache -->> backend: kb_text [10ms Cache Hit]
deactivate cache

backend ->> cache: get_cached_meanings(db_name)
activate cache
cache -->> backend: meanings_text [10ms Cache Hit]
deactivate cache

note over backend: **Phase 2 & 3: ReAct Loop (Parallel)**
par Ambiguity Detection
  backend ->> llm: check_ambiguity(question, schema, kb)
  activate llm
  note right of llm: ambiguity_prompt.txt
  llm ->> openai: POST /chat/completions\n  model: gpt-5-2\n  temperature: 0.3
  openai -->> llm: {is_ambiguous: bool,\n   ambiguities: [],\n   clarifying_questions: []}
  llm -->> backend: AmbiguityResult [500ms]
  deactivate llm
else SQL Generation (ReAct)
  backend ->> llm: generate_sql_with_react(question, schema, kb)
  activate llm
  note right of llm: few_shot_prompt.txt
  
  loop ReAct Iterations (max: 2)
    llm ->> openai: Thoughtâ†’Actionâ†’Observation\n  model: gpt-5-2\n  max_tokens: 2000
    openai -->> llm: response (thought, action, sql)
    llm ->> vector: Vector search for action\n  (if action requires retrieval)
    vector -->> llm: retrieved_chunks
  end
  
  llm -->> backend: {generated_sql,\n   explanation,\n   confidence: 0.0-1.0} [2-4s]
  deactivate llm
end

note over backend: **Phase 4: 3-Layer Validation**
backend ->> guard: Level 1: SQL Guard\n  enforce_safety()
activate guard
note right of guard: Regex-based (10ms)\n  Check: DELETE? DROP? INSERT?\n  Check: Known tables only\n  Check: Single statement
guard -->> backend: {valid: bool,\n   errors: []}
deactivate guard

alt guard.valid == false
  backend -->> frontend: QueryResponse(\n    error="SQL Injection detected",\n    error_code=400)
  activate frontend
  frontend ->> user: âŒ Security Error
  deactivate frontend
end

backend ->> llm: Level 2: LLM Semantic Validation
activate llm
note right of llm: validation_prompt.txt
llm ->> openai: "ÃœberprÃ¼fe Semantik:\n  1. Entspricht SQL der Frage?\n  2. Sind JOINs korrekt?\n  3. Spalten-Qualifizierung OK?"
openai -->> llm: {is_valid: bool,\n   issues: [],\n   severity: "low|high"}
llm -->> backend: ValidationResult [1-2s]
deactivate llm

alt validation.severity == "high"
  backend -->> frontend: QueryResponse(\n    error="Semantic validation failed")
end

note over backend: **Phase 5: Query Execution + Paging**
backend ->> db: execute_with_pagination(\n  sql, page=1, page_size=100)
activate db
note right of db: SQLite DB File\n  database/credit.sqlite
db ->> db: SELECT ... LIMIT 100 OFFSET 0
db -->> backend: {rows: [{...}, {...}],\n   total_rows: int,\n   total_pages: int}
deactivate db

backend ->> cache: cache_query_result(\n  question_hash,\n  rows,\n  ttl=5min)
activate cache
note right of cache: Query Results Cache\n  LRU + TTL
cache -->> backend: âœ“
deactivate cache

backend ->> cache: cache_session(\n  session_id, sql,\n  ttl=1h)
activate cache
note right of cache: Session Cache for Paging
cache -->> backend: âœ“
deactivate cache

note over backend: **Phase 6: Summarization (Optional)**
backend ->> llm: summarize_results(question, sql, top_rows)
activate llm
note right of llm: summarization_prompt.txt
llm ->> openai: "Schreibe 2-3 SÃ¤tze\n  Ã¼ber diese Ergebnisse"
openai -->> llm: summary_text
llm -->> backend: {summary: str} [1-2s]
deactivate llm

deactivate backend

backend -->> frontend: QueryResponse(\n  question: str,\n  generated_sql: str,\n  results: [{...}, {...}],\n  row_count: int,\n  total_pages: int,\n  summary: str,\n  validation: ValidationResult,\n  metrics: {latency_ms, tokens_used})

activate frontend
frontend ->> frontend: render_results()
note right of frontend: Display SQL (with syntax highlight)\n  Display Table (paginated)\n  Display Summary\n  Display Validation Info
frontend ->> user: âœ… Full Result Display\n  [SQL Code] | [Table 1/n] | [Summary]
deactivate frontend

note bottom
  **Performance Summary:**
  ðŸ“Š Cache Hit (95%): 45ms
  ðŸ“Š Cache Miss (5%): 6.2s
  ðŸ“Š Latency Breakdown:
     - Phase 1: 500ms | 10ms (cache)
     - Phase 2-3: 4s (parallel)
     - Phase 4: 2s
     - Phase 5: 1-10s
     - Phase 6: 1-2s (optional)
  ðŸ“Š Cost: $0.12-0.18/query (73% reduction from GPT-4o-mini)
  ðŸ“Š Accuracy: 88%
  ðŸ“Š Tokens/Query: 1400 avg (was 5500 with GPT-4o-mini)
end note

legend right
  |<#bbdefb> Frontend |
  |<#e3f2fd> Backend |
  |<#f3e5f5> AI/LLM |
  |<#fff3e0> Database |
end legend

@enduml
